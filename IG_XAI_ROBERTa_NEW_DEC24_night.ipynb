{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tommy-Adisa/Explainable-Suicide-Risk-Detection-with-Transformers/blob/main/IG_XAI_ROBERTa_NEW_DEC24_night.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MVHt8h2O1Mo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avmGWfLP6pax"
      },
      "outputs": [],
      "source": [
        "!pip install -U \\\n",
        "transformers \\\n",
        "torch \\\n",
        "scikit-learn \\\n",
        "pandas \\\n",
        "numpy==1.26.4 \\\n",
        "tqdm \\\n",
        "contractions \\\n",
        "captum \\\n",
        "matplotlib \\\n",
        "seaborn \\\n",
        "wordcloud\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wordcloud\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"WordCloud:\", wordcloud.__version__)\n"
      ],
      "metadata": {
        "id": "4CLQ2uwNl1Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNpl1YxjMuX4"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers torch scikit-learn pandas numpy tqdm contractions captum wordcloud seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4XHjdxZVKxv"
      },
      "outputs": [],
      "source": [
        "#!pip install wordcloud\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtmjP-HBNWv-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "RobertaTokenizer, RobertaForSequenceClassification,\n",
        "Trainer,\n",
        "TrainingArguments\n",
        ")\n",
        "#from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8k6e0YM9Aeg"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "#df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/explainable_suicide_detection/data/raw/Suicide_Detection.csv')\n",
        "#df.head()\n",
        "\n",
        "#print(df['class'].value_counts())\n",
        "#print(len(df))\n",
        "\n",
        "# Load dataset\n",
        "df_all = pd.read_csv('/content/drive/My Drive/Colab Notebooks/explainable_suicide_detection/data/raw/Suicide_Detection.csv')\n",
        "df_all.head()\n",
        "\n",
        "print(df_all['class'].value_counts())\n",
        "print(len(df_all))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mp47fVq8DdX"
      },
      "source": [
        "#NOTE SECTION BELLOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9qM40ec72gC"
      },
      "source": [
        "#WE NEED TO DOWNSAMPLE THE DATASET TO HELP FOR THE CODING. AFTER ALL IS DONE, THEN WE WILL REMOVE THIS SECTION TO RUN IT ON ALL THE CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T963Ei7v8UVP"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all = df_all.drop_duplicates(subset='text')\n",
        "\n",
        "N = 1000\n",
        "df = df_all.groupby('class').apply(lambda x: x.sample(N, random_state=42)).reset_index(drop=True)\n",
        "\n",
        "df.to_csv(\"/content/drive/My Drive/Colab Notebooks/explainable_suicide_detection/data/reduced/suicide_data_small.csv\", index=False)\n",
        "\n",
        "print(df['class'].value_counts())\n",
        "print(len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENAeC_T9NsgY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Confirm dataset structure\n",
        "print(df.columns)\n",
        "print(df['class'].value_counts())\n",
        "print(df.shape)\n",
        "list(df.columns)\n",
        "\n",
        "df = df[[\"text\", \"class\"]]\n",
        "label_map = {\"non-suicide\": 0, \"suicide\": 1}\n",
        "df[\"label\"] = df[\"class\"].map(label_map)\n",
        "\n",
        "#PRINT FIRTH 10\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5KEj41YR9Ux"
      },
      "source": [
        "###THIS IS TO SPLIT THE DATASET BEFORE CLEANING."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFlZV5oYSMl1"
      },
      "source": [
        "“Minimal text preprocessing was applied to preserve linguistic and emotional cues essential for transformer-based models. Unlike traditional machine learning pipelines, aggressive text normalisation was avoided to ensure meaningful attention distributions and faithful Integrated Gradient attributions.”\n",
        "\n",
        "When we clean before spliting, we let  information from test data influence training data\n",
        "\n",
        "Cleaning before splitting, cause risk of:\n",
        "\n",
        "Vocabulary leakage, Distribution leakage, Over-optimistic performance, Invalid explainability results\n",
        "\n",
        "BERT was trained on raw, messy, natural language, including:\n",
        "\n",
        "punctuation, repetition, emotional intensity, informal text, subwords, casing (for cased models). IF we remove too much hurts performance and explainability. Therfore, for safe cleaning and minimal cleaning, we will perform: Lowercasing,Removing URLs\n",
        "\n",
        "Removing HTML tags\n",
        "\n",
        "Expanding contractions\n",
        "\n",
        "Removing emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQH7qgy0bgk9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import contractions\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwInzQaEbj7A"
      },
      "source": [
        "###Clean functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpWiEn7Oh4tc"
      },
      "outputs": [],
      "source": [
        "def clean_contractions(text):\n",
        "    try:\n",
        "        return contractions.fix(text)\n",
        "    except:\n",
        "        return text  # fallback if contractions fails\n",
        "\n",
        "def remove_html_tags_func(text):\n",
        "    return re.sub(r'<.*?>', '', text)\n",
        "\n",
        "def remove_url_func(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U00002500-\\U00002BEF\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U0001F900-\\U0001F9FF\"\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"\n",
        "        u\"\\U0001F018-\\U0001F270\"\n",
        "        u\"\\U0001F650-\\U0001F67F\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE,\n",
        "    )\n",
        "    return emoji_pattern.sub(r\"\", text)\n",
        "\n",
        "def clean_text_bert(text):\n",
        "    text = str(text) if text else \"\"\n",
        "    text = text.strip()\n",
        "    if text == \"\":\n",
        "        return \"\"\n",
        "    text = clean_contractions(text)\n",
        "    text = remove_html_tags_func(text)\n",
        "    text = remove_url_func(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ8XMMLhb0uB"
      },
      "source": [
        "###Stratified Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMy9OpVeVZ6L"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(\n",
        "    df.copy(),\n",
        "    test_size=0.2,\n",
        "    stratify=df['label'],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bO1351giJGp"
      },
      "source": [
        "####Fill NaN & ensure strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn7U-6WUfr9E"
      },
      "outputs": [],
      "source": [
        "train_df['text'] = train_df['text'].fillna(\"\").astype(str)\n",
        "val_df['text']   = val_df['text'].fillna(\"\").astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTNhEAvmb_z_"
      },
      "source": [
        "####Apply Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2fABk05dFwF"
      },
      "outputs": [],
      "source": [
        "train_df['clean_text'] = train_df['text'].apply(clean_text_bert)\n",
        "val_df['clean_text']   = val_df['text'].apply(clean_text_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHZT_iCmi9n2"
      },
      "outputs": [],
      "source": [
        "# Sample 5 random rows from each class\n",
        "sample_class_1 = train_df[train_df['label'] == 1].sample(5, random_state=42)\n",
        "sample_class_0 = train_df[train_df['label'] == 0].sample(5, random_state=42)\n",
        "\n",
        "# Combine them and reset index\n",
        "sample_train_df = pd.concat([sample_class_1, sample_class_0]).reset_index(drop=True)\n",
        "\n",
        "# Display only the columns you want, like with head()\n",
        "sample_train_df[[ 'text', 'clean_text', 'class', 'label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiefe1g5kevf"
      },
      "source": [
        "###Extract lists for tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS4CLEwnkbVM"
      },
      "outputs": [],
      "source": [
        "train_texts  = train_df['clean_text'].tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "\n",
        "val_texts    = val_df['clean_text'].tolist()\n",
        "val_labels   = val_df['label'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwHJnZ9alKrB"
      },
      "source": [
        "###Tokenize with Hugging Face BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNdvM-3MlQuh"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "val_encodings   = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp39-ixrrZ3_"
      },
      "source": [
        "###Dataset Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y35MwI-XsKfL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWdL9bgxVsI"
      },
      "source": [
        "####This is to Define batch size and device. We will use the approach of effective batch size batch_size = 8, gradient_accumulation_steps = 2, Effective batch size = 8 * 2 = 16. The optimizer will only update weights after accumulating gradients over 2 batches, effectively mimicking a batch of 16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heo8tMKsxWn0"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "gradient_accumulation_steps = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2brugxXrrZbs"
      },
      "outputs": [],
      "source": [
        "class TokenizedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn_78h7Csl1c"
      },
      "source": [
        "##Prepare Tokenizers and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsy8IdRxssFq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = TokenizedDataset(train_encodings, train_labels)\n",
        "val_dataset   = TokenizedDataset(val_encodings, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B45wLWosvTlk"
      },
      "source": [
        "###This is to Wrap into DataLoader for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzxmf5DsvV-e"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccgVdiELywKi"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(\"input_ids:\", batch['input_ids'].shape)        # [batch_size, max_len]\n",
        "print(\"attention_mask:\", batch['attention_mask'].shape)\n",
        "print(\"labels:\", batch['labels'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi-g3Vj7y9Ij"
      },
      "source": [
        "###Difine model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxzkJbppR6wp"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "\n",
        "num_labels = 2  # binary classification\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=num_labels ,\n",
        "    output_attentions=True\n",
        ")\n",
        "\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZiLPbGB2nYl"
      },
      "outputs": [],
      "source": [
        "#from transformers import RobertaForSequenceClassification\n",
        "\n",
        "\n",
        "#num_labels = 2  # binary classification\n",
        "\n",
        "#model = RobertaForSequenceClassification.from_pretrained(\n",
        " #   \"roberta-base\",\n",
        "  #  num_labels=num_labels ,\n",
        "   # output_attentions=True\n",
        "#)\n",
        "\n",
        "#model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAtDDHH-22GK"
      },
      "source": [
        "####This is to difine the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7z8Yo83271R"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=2e-5,\n",
        "    eps=1e-8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp1W4nxb4BsK"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 3\n",
        "total_steps = len(train_loader) * epochs // gradient_accumulation_steps\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZjCsNropBcM"
      },
      "outputs": [],
      "source": [
        "print(next(model.parameters()).device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJQTRtLR0agW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    loop = tqdm(\n",
        "        enumerate(train_loader),\n",
        "        total=len(train_loader),\n",
        "        desc=f\"Epoch {epoch+1}\",\n",
        "        leave=True\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, batch in loop:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        #  tqdm live update\n",
        "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} finished. Average loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKgjsoMPCvVw"
      },
      "source": [
        "#EVALUATION OF ROBERTa#\n",
        "##1. Confusion matrix 2. Precision, recall, F1 3. Classification report 4. Store correctly predicted suicide samples 5. Store correctly predicted non-suicide samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gByn3FVb09pc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDSIRQZqDPOY"
      },
      "outputs": [],
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "correct_suicide_samples = []\n",
        "correct_nonsuicide_samples = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating\", leave=True):\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Store correctly predicted samples\n",
        "        for i in range(len(labels)):\n",
        "            if preds[i] == labels[i]:\n",
        "                if labels[i].item() == 1:\n",
        "                    correct_suicide_samples.append(input_ids[i].cpu())\n",
        "                else:\n",
        "                    correct_nonsuicide_samples.append(input_ids[i].cpu())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkh3o1bIEq86"
      },
      "source": [
        "##CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw4JVDI7EWkW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Non-Suicide', 'Suicide'],\n",
        "            yticklabels=['Non-Suicide', 'Suicide'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('ROBERTa Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhBIw6iPE1Bz"
      },
      "source": [
        "#Precision, Recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK2HegZQEpV1"
      },
      "outputs": [],
      "source": [
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    average='binary'\n",
        ")\n",
        "\n",
        "print(f\"ROBERTa_Precision: {precision:.4f}\")\n",
        "print(f\"ROBERTa_Recall:    {recall:.4f}\")\n",
        "print(f\"ROBERTa_F1-score:  {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd_iTCLlFLy2"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    classification_report(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        target_names=['Non-Suicide', 'Suicide']\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ntR8CbjFbp-"
      },
      "outputs": [],
      "source": [
        "def decode_samples(token_ids, tokenizer, max_samples=50):\n",
        "    texts = []\n",
        "    for ids in token_ids[:max_samples]:\n",
        "        text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "        texts.append(text)\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUHzgGjZFf9q"
      },
      "outputs": [],
      "source": [
        "correct_suicide_texts = decode_samples(\n",
        "    correct_suicide_samples,\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "correct_nonsuicide_texts = decode_samples(\n",
        "    correct_nonsuicide_samples,\n",
        "    tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWGeMF2wFli9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame({'text': correct_suicide_texts}).to_csv(\n",
        "    \"/content/drive/My Drive/Colab Notebooks/explainable_suicide_detection/correct_suicide_samples_roberta.csv\", index=False\n",
        ")\n",
        "\n",
        "pd.DataFrame({'text': correct_nonsuicide_texts}).to_csv(\n",
        "    '/content/drive/My Drive/Colab Notebooks/explainable_suicide_detection/correct_nonsuicide_samples_roberta.csv', index=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4L55-KWHxT7"
      },
      "source": [
        "#EXPLAINABILITY AI ATTENTION + INTEGRATED GRADIENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGNxxHghIT4G"
      },
      "source": [
        "#NOTE\n",
        "\n",
        "### What attention shows\n",
        "##Attention answers: “Which words did ROBERTa focus on when forming its internal representation?”\n",
        "In practice:\n",
        "•\tAttention scores show token-to-token influence\n",
        "•\tThey highlight contextual importance\n",
        "•\tThey are model-internal signals, not causal proofs\n",
        "\n",
        "###Important limitation\n",
        "##Attention ≠ Explanation by itself\n",
        "This is a known research conclusion:\n",
        "•\tHigh attention ≠ high importance\n",
        "\n",
        "##Attention shows where the model looks, not why the output changes\n",
        " So attention alone is insufficient for explainability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPVi6MzwI0OJ"
      },
      "source": [
        "##TASK TO DO NEXT\n",
        "####combine attention mechanisms and Integrated Gradients to identify both model focus and causal word-level contributions, highlighting their alignment and divergence”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHMOpIP5PDjb"
      },
      "source": [
        "###Select sample from sucide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93KmU8SzPKU1"
      },
      "source": [
        "####Tokinize the sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqnhD5PbOi5c"
      },
      "outputs": [],
      "source": [
        "#pick a sample\n",
        "sample_text = correct_suicide_texts[8]\n",
        "\n",
        "inputs = tokenizer(\n",
        "    sample_text,\n",
        "    return_tensors='pt',\n",
        "    truncation=True,\n",
        "    max_length=128\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzha6grxPdGY"
      },
      "source": [
        "###Forward pass with attention extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jugoLx7jPcTo"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "attentions = outputs.attentions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YluNT7rAPoJ3"
      },
      "source": [
        "###Aggregate attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BzDiPTyPryn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Average heads first → shape: (layers, tokens, tokens)\n",
        "layer_attentions = torch.stack([layer.mean(dim=1).squeeze(0) for layer in attentions])\n",
        "\n",
        "# Average across layers → overall attention\n",
        "avg_attention = layer_attentions.mean(dim=0)  # shape: (tokens, tokens)\n",
        "\n",
        "# CLS token attention to all tokens\n",
        "cls_attention = avg_attention[0]  # shape: (tokens,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhRciyLjaxNo"
      },
      "source": [
        "####Merge WordPiece tokens into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCAGWgmvayqp"
      },
      "outputs": [],
      "source": [
        "def merge_wordpieces(tokens, attention_scores):\n",
        "    words = []\n",
        "    word_scores = []\n",
        "\n",
        "    current_word = ''\n",
        "    current_score = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for token, score in zip(tokens, attention_scores):\n",
        "        if token.startswith('##'):\n",
        "            current_word += token[2:]\n",
        "            current_score += score\n",
        "            count += 1\n",
        "        else:\n",
        "            if current_word != '':\n",
        "                words.append(current_word)\n",
        "                word_scores.append(current_score / count)\n",
        "            current_word = token\n",
        "            current_score = score\n",
        "            count = 1\n",
        "\n",
        "    # Append last word\n",
        "    words.append(current_word)\n",
        "    word_scores.append(current_score / count)\n",
        "\n",
        "    return words, torch.tensor(word_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHsfxkwHa5wF"
      },
      "outputs": [],
      "source": [
        "tokens_raw = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "# Clean tokens\n",
        "tokens = [t.replace(\"Ġ\", \"\") for t in tokens_raw]\n",
        "\n",
        "\n",
        "words, word_attention = merge_wordpieces(tokens, cls_attention.cpu())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox6kaE9GVEww"
      },
      "source": [
        "###Heatmap of attention across tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORvKK3KOXCR6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_k = 15\n",
        "top_indices = np.argsort(word_attention.numpy())[::-1][:top_k]\n",
        "top_words = [words[i] for i in top_indices]\n",
        "\n",
        "# Fix negative-stride issue\n",
        "word_attention_np = word_attention.numpy()\n",
        "top_values = word_attention_np[top_indices]\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(top_words, top_values, color='blue')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Attention Weight\")\n",
        "plt.title(\"Top-K Words by CLS Attention\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiYAhSnEbmpX"
      },
      "outputs": [],
      "source": [
        "word_attention_np = word_attention.numpy()\n",
        "top_values = word_attention_np[top_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLuRt6PebGof"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def highlight_text(words, attention_scores):\n",
        "    # Normalize\n",
        "    scores = (attention_scores - attention_scores.min()) / (attention_scores.max() - attention_scores.min())\n",
        "    cmap = cm.get_cmap('Reds')\n",
        "    html = ''\n",
        "    for word, score in zip(words, scores):\n",
        "        color = cm.colors.to_hex(cmap(score))\n",
        "        html += f'<span style=\"background-color:{color}\">{word} </span>'\n",
        "    return html\n",
        "\n",
        "html = highlight_text(words, word_attention)\n",
        "display(HTML(html))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aeo4AsxZXpd0"
      },
      "outputs": [],
      "source": [
        "cls_numpy = cls_attention.cpu().numpy()\n",
        "top_values = cls_numpy[top_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYpndqmlVGEr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_k = 15\n",
        "\n",
        "# Convert to NumPy and fix negative-stride issues\n",
        "word_attention_np = word_attention.numpy()\n",
        "\n",
        "# Get top-K indices\n",
        "top_indices = np.argsort(word_attention_np)[::-1][:top_k]\n",
        "\n",
        "# Get top-K words and their attention scores\n",
        "top_words = [words[i] for i in top_indices]\n",
        "top_values = word_attention_np[top_indices]\n",
        "\n",
        "# Plot like the first style (line plot with markers)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(top_values, marker='o', linestyle='-', color='royalblue')\n",
        "plt.xticks(range(top_k), top_words, rotation=45)\n",
        "plt.ylabel(\"Attention Weight\")\n",
        "plt.title(f\"Top-{top_k} Words by CLS Attention\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaKWsispVWwY"
      },
      "outputs": [],
      "source": [
        "token_importance = list(zip(tokens, cls_attention.cpu().numpy()))\n",
        "\n",
        "# Sort by importance\n",
        "token_importance = sorted(token_importance, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for token, score in token_importance[:10]:\n",
        "    print(f\"{token}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuznGHS3VpRt"
      },
      "outputs": [],
      "source": [
        "combined_attention = torch.zeros_like(attentions[0][0][0])\n",
        "\n",
        "for layer in attentions:\n",
        "    for head in layer[0]:\n",
        "        combined_attention += head\n",
        "\n",
        "combined_attention /= (len(attentions) * layer.shape[1])\n",
        "\n",
        "cls_combined = combined_attention[0]\n",
        "cls_combined /= cls_combined.sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPgpg6lsxzRd"
      },
      "outputs": [],
      "source": [
        "layer = -1\n",
        "head = 3\n",
        "\n",
        "head_attention = attentions[layer][0, head, 0].detach().cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.plot(head_attention)\n",
        "plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
        "plt.title(f\"Layer {layer}, Head {head} CLS Attention\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIB5u9FRzxzP"
      },
      "source": [
        "###“Attention visualisation showed dominant focus on special tokens ([CLS], [SEP]) with diffuse distribution across content words. This supports prior findings that attention weights do not reliably correspond to feature importance. Therefore, Integrated Gradients were employed for faithful attribution.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL0kmht20Y7j"
      },
      "source": [
        "#INTEGRATED GRADIENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57KgqJcGktv3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from captum.attr import IntegratedGradients\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlRJVv9C0qCV"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i7zBntD0tbz"
      },
      "source": [
        "###IG inpute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXQmvZ1N0wtq"
      },
      "outputs": [],
      "source": [
        "def prepare_ig_inputs(input_ids, tokenizer):\n",
        "    \"\"\"\n",
        "    Creates attention mask and PAD baseline IDs for IG.\n",
        "    \"\"\"\n",
        "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
        "\n",
        "    baseline_ids = torch.full_like(\n",
        "        input_ids,\n",
        "        tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    return attention_mask, baseline_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbiu3ZyO01_T"
      },
      "source": [
        "###Get RoBERTa Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08pwV1BD1Bfj"
      },
      "outputs": [],
      "source": [
        "def get_roberta_embeddings(model, input_ids):\n",
        "    \"\"\"\n",
        "    Returns RoBERTa token embeddings for IG.\n",
        "    \"\"\"\n",
        "    return model.roberta.embeddings(input_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgbHxiQ81K5Z"
      },
      "source": [
        "###Forward Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw4bl_Tk1McL"
      },
      "outputs": [],
      "source": [
        "def forward_func_roberta(embeddings, attention_mask):\n",
        "    outputs = model.roberta(\n",
        "        inputs_embeds=embeddings,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: pass full hidden states\n",
        "    logits = model.classifier(outputs.last_hidden_state)\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmA8OzIO1a8I"
      },
      "source": [
        "###Run Integrated Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHqusruq1gFo"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "ig = IntegratedGradients(forward_func_roberta)\n",
        "\n",
        "# Pick ONE correctly predicted suicide sample\n",
        "sample_ids = correct_suicide_samples[0].unsqueeze(0).to(device)\n",
        "\n",
        "# Prepare inputs\n",
        "attention_mask, baseline_ids = prepare_ig_inputs(sample_ids, tokenizer)\n",
        "\n",
        "# Embeddings\n",
        "input_embeddings = get_roberta_embeddings(model, sample_ids)\n",
        "baseline_embeddings = get_roberta_embeddings(model, baseline_ids)\n",
        "\n",
        "input_embeddings.requires_grad_(True)\n",
        "\n",
        "# IG attribution\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=input_embeddings,\n",
        "    baselines=baseline_embeddings,\n",
        "    additional_forward_args=(attention_mask,),\n",
        "    target=1,  # suicide class\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "print(\"Attributions shape:\", attributions.shape)\n",
        "print(\"Convergence delta:\", delta.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WussDSzq1rmY"
      },
      "source": [
        "###Token-Level Attribution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Token cleaning function"
      ],
      "metadata": {
        "id": "zwuEO3NoAZgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "def clean_roberta_token_keep_boundary(token):\n",
        "    if token in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
        "        return \"\"\n",
        "    if all(char in string.punctuation for char in token.replace(\"Ġ\", \"\")):\n",
        "        return \"\"\n",
        "    return token\n"
      ],
      "metadata": {
        "id": "BOnYt0sPAWG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Merge subwords into words"
      ],
      "metadata": {
        "id": "uNUtVVFmE7iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_roberta_subwords(tokens, scores):\n",
        "    merged_tokens = []\n",
        "    merged_scores = []\n",
        "\n",
        "    current_word = \"\"\n",
        "    current_score = 0.0\n",
        "\n",
        "    for tok, score in zip(tokens, scores):\n",
        "        if tok == \"\":\n",
        "            continue\n",
        "\n",
        "        if tok.startswith(\"Ġ\"):\n",
        "            # Start of a NEW word\n",
        "            if current_word != \"\":\n",
        "                merged_tokens.append(current_word)\n",
        "                merged_scores.append(current_score)\n",
        "\n",
        "            current_word = tok.replace(\"Ġ\", \"\")\n",
        "            current_score = score\n",
        "        else:\n",
        "            # Continuation subword\n",
        "            current_word += tok\n",
        "            current_score += score\n",
        "\n",
        "    if current_word != \"\":\n",
        "        merged_tokens.append(current_word)\n",
        "        merged_scores.append(current_score)\n",
        "\n",
        "    return merged_tokens, merged_scores\n"
      ],
      "metadata": {
        "id": "w76V2qFqE9HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fG9B9GnHqvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####RoBERTa subword attributions were aggregated to word-level importance by detecting word boundaries via the Ġ prefix and summing subword attributions accordingly."
      ],
      "metadata": {
        "id": "yEtAQpOPBrgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This is to Build token–IG table"
      ],
      "metadata": {
        "id": "AxF5AsLwB-Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean tokens but KEEP Ġ\n",
        "clean_tokens = [clean_roberta_token_keep_boundary(t) for t in tokens]\n",
        "\n",
        "# Ensure scores are numpy\n",
        "scores = (\n",
        "    token_importance.detach().cpu().numpy()\n",
        "    if torch.is_tensor(token_importance)\n",
        "    else token_importance\n",
        ")\n",
        "\n",
        "merged_tokens, merged_scores = merge_roberta_subwords(clean_tokens, scores)\n",
        "\n",
        "df_words = pd.DataFrame({\n",
        "    \"token\": merged_tokens,\n",
        "    \"ig_score\": merged_scores\n",
        "})\n",
        "\n",
        "df_words[\"abs_ig\"] = df_words[\"ig_score\"].abs()\n"
      ],
      "metadata": {
        "id": "-13-BFCFCBh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "#df_words = df_words[~df_words[\"token\"].isin(ENGLISH_STOP_WORDS)]\n"
      ],
      "metadata": {
        "id": "iaWiN3mhH-QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sort and Select top 20"
      ],
      "metadata": {
        "id": "DKr6ru43CM31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top20_clean = (\n",
        "    df_words\n",
        "    .sort_values(\"abs_ig\", ascending=False)\n",
        "    .head(20)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "top20_clean.index += 1\n",
        "\n",
        "print(\"\\nTop-20 IG Words (RoBERTa — Cleaned & Merged):\\n\")\n",
        "for i, row in top20_clean.iterrows():\n",
        "    print(\n",
        "        f\"{i:>2}. {row.token:<20} \"\n",
        "        f\"IG={row.ig_score:+.4f} | |IG|={row.abs_ig:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "E1_PwpfTCRvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiline Integrated Gradients heatmap for RoBERTa"
      ],
      "metadata": {
        "id": "6_S4sFNjT1ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This is to Prepare token-level IG scores"
      ],
      "metadata": {
        "id": "jMjCdMfrUJKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum across embedding dimension\n",
        "token_importance = attributions.sum(dim=-1).squeeze(0)\n",
        "\n",
        "# Detach & move to CPU\n",
        "token_importance = token_importance.detach().cpu().numpy()\n",
        "\n",
        "# Convert token IDs → tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(sample_ids.squeeze(0))\n"
      ],
      "metadata": {
        "id": "R08GS9ONT459"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clean tokens for RoBERTa heatmap"
      ],
      "metadata": {
        "id": "ZGBzbtZvUVcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def clean_roberta_token_for_heatmap(token):\n",
        "    if token in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
        "        return \"\"\n",
        "    token = token.replace(\"Ġ\", \"\")  # remove space marker\n",
        "    if token == \"\":\n",
        "        return \"\"\n",
        "    if all(char in string.punctuation for char in token):\n",
        "        return \"\"\n",
        "    return token\n"
      ],
      "metadata": {
        "id": "6S2GEAujId-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens = [clean_roberta_token_for_heatmap(t) for t in tokens]\n"
      ],
      "metadata": {
        "id": "bTiFgzL4Id25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multiline IG heatmap"
      ],
      "metadata": {
        "id": "Vy5YTPWtUiKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def visualize_token_heatmap_multiline(\n",
        "    tokens,\n",
        "    scores,\n",
        "    title=\"Integrated Gradients Heatmap (RoBERTa)\",\n",
        "    tokens_per_row=20\n",
        "):\n",
        "    # Filter empty tokens\n",
        "    filtered = [(t, s) for t, s in zip(tokens, scores) if t != \"\"]\n",
        "    tokens, scores = zip(*filtered)\n",
        "\n",
        "    scores = np.array(scores)\n",
        "\n",
        "    # Normalize scores for color intensity\n",
        "    scores_norm = scores / (np.max(np.abs(scores)) + 1e-8)\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "    num_rows = math.ceil(num_tokens / tokens_per_row)\n",
        "\n",
        "    # Pad tokens and scores\n",
        "    pad_len = num_rows * tokens_per_row - num_tokens\n",
        "    tokens_padded = list(tokens) + [\"\"] * pad_len\n",
        "    scores_padded = np.pad(scores_norm, (0, pad_len), constant_values=0)\n",
        "\n",
        "    token_matrix = np.array(tokens_padded).reshape(num_rows, tokens_per_row)\n",
        "    score_matrix = scores_padded.reshape(num_rows, tokens_per_row)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(tokens_per_row * 0.6, num_rows * 0.6))\n",
        "    im = plt.imshow(score_matrix, cmap=\"RdBu_r\", aspect=\"auto\", vmin=-1, vmax=1)\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        for j in range(tokens_per_row):\n",
        "            tok = token_matrix[i, j]\n",
        "            if tok != \"\":\n",
        "                plt.text(j, i, tok, ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.colorbar(im, fraction=0.02, pad=0.02, label=\"Normalized IG score\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "YI3Kf__3Ujvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_token_heatmap_multiline(\n",
        "    clean_tokens,\n",
        "    token_importance,\n",
        "    title=\"Multiline Integrated Gradients Heatmap (RoBERTa)\",\n",
        "    tokens_per_row=20\n",
        ")\n"
      ],
      "metadata": {
        "id": "l6oyFlByUtxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Token-level attributions were visualized using a multiline heatmap, where Integrated Gradients scores were summed across the embedding dimension and normalized for visual comparison.”"
      ],
      "metadata": {
        "id": "ZAuJSXtNVoru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####RoBERTa subword merging"
      ],
      "metadata": {
        "id": "-EYy67KBa-7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_roberta_tokens(tokens, scores):\n",
        "    merged_tokens = []\n",
        "    merged_scores = []\n",
        "\n",
        "    current_word = \"\"\n",
        "    current_score = 0.0\n",
        "\n",
        "    for tok, score in zip(tokens, scores):\n",
        "        if tok in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
        "            continue\n",
        "\n",
        "        if tok.startswith(\"Ġ\"):\n",
        "            # New word\n",
        "            if current_word:\n",
        "                merged_tokens.append(current_word)\n",
        "                merged_scores.append(current_score)\n",
        "            current_word = tok[1:]   # remove Ġ\n",
        "            current_score = score\n",
        "        else:\n",
        "            # Continuation subword\n",
        "            current_word += tok\n",
        "            current_score += score\n",
        "\n",
        "    if current_word:\n",
        "        merged_tokens.append(current_word)\n",
        "        merged_scores.append(current_score)\n",
        "\n",
        "    return merged_tokens, merged_scores\n"
      ],
      "metadata": {
        "id": "4DOxiW7vbBe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stopwords"
      ],
      "metadata": {
        "id": "6oNbpCJebIaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n"
      ],
      "metadata": {
        "id": "xprv6w6hbLpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This is to Collect IG word scores"
      ],
      "metadata": {
        "id": "kYy8w4PfbRuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from captum.attr import IntegratedGradients\n",
        "import torch\n",
        "\n",
        "def collect_ig_scores_roberta(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    correct_samples,\n",
        "    target_class,\n",
        "    max_samples=50\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    word_scores = defaultdict(float)\n",
        "    word_counts = defaultdict(int)\n",
        "\n",
        "    ig = IntegratedGradients(forward_func_roberta)\n",
        "\n",
        "    for sample_ids in correct_samples[:max_samples]:\n",
        "        sample_ids = sample_ids.unsqueeze(0).to(device)\n",
        "\n",
        "        attention_mask = (sample_ids != tokenizer.pad_token_id).long()\n",
        "\n",
        "        baseline_ids = torch.full_like(\n",
        "            sample_ids, tokenizer.pad_token_id\n",
        "        ).to(device)\n",
        "\n",
        "        # Embeddings\n",
        "        input_embeds = model.roberta.embeddings(sample_ids)\n",
        "        baseline_embeds = model.roberta.embeddings(baseline_ids)\n",
        "        input_embeds.requires_grad_(True)\n",
        "\n",
        "        # Integrated Gradients\n",
        "        attributions, _ = ig.attribute(\n",
        "            inputs=input_embeds,\n",
        "            baselines=baseline_embeds,\n",
        "            additional_forward_args=(attention_mask,),\n",
        "            target=target_class,\n",
        "            return_convergence_delta=True\n",
        "        )\n",
        "\n",
        "        # Token-level IG\n",
        "        token_importance = attributions.sum(dim=-1).squeeze(0)\n",
        "        token_importance = token_importance / torch.norm(token_importance)\n",
        "\n",
        "        tokens = tokenizer.convert_ids_to_tokens(sample_ids.squeeze(0))\n",
        "\n",
        "        words, scores = merge_roberta_subwords(\n",
        "            tokens,\n",
        "            token_importance.detach().cpu().numpy()\n",
        "        )\n",
        "\n",
        "        for w, s in zip(words, scores):\n",
        "            w = w.lower()\n",
        "            if len(w) < 3 or not w.isalpha():\n",
        "                continue\n",
        "\n",
        "            if s > 0:  # positive IG for suicide class\n",
        "                word_scores[w] += s\n",
        "                word_counts[w] += 1\n",
        "\n",
        "    mean_word_scores = {\n",
        "        w: word_scores[w] / word_counts[w]\n",
        "        for w in word_scores\n",
        "        if word_counts[w] >= 2\n",
        "    }\n",
        "\n",
        "    return mean_word_scores\n"
      ],
      "metadata": {
        "id": "bW5CHl8IbUrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Cloud"
      ],
      "metadata": {
        "id": "G7Kxx5_NbjlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# PATCH numpy.asarray to ignore `copy`\n",
        "_old_asarray = np.asarray\n",
        "def _patched_asarray(a, *args, **kwargs):\n",
        "    kwargs.pop(\"copy\", None)\n",
        "    return _old_asarray(a, *args, **kwargs)\n",
        "\n",
        "np.asarray = _patched_asarray\n"
      ],
      "metadata": {
        "id": "OBKulF2voTZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ig_wordcloud(word_scores, title):\n",
        "    wc = WordCloud(\n",
        "        width=1000,\n",
        "        height=400,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"Reds\",\n",
        "        max_words=100\n",
        "    ).generate_from_frequencies(word_scores)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AuDYoiI9bl0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Roberta Sucide class"
      ],
      "metadata": {
        "id": "YEY8vvIGbrtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"WordCloud:\", wordcloud.__version__)\n"
      ],
      "metadata": {
        "id": "g--2_BUqmD1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suicide_word_scores_roberta = collect_ig_scores_roberta(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    correct_suicide_samples,\n",
        "    target_class=1,\n",
        "    max_samples=50\n",
        ")\n",
        "plot_ig_wordcloud(\n",
        "    suicide_word_scores_roberta,\n",
        "    title=\"IG-Weighted Word Cloud — Suicide Class (RoBERTa)\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "wOUL0LYPbw3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Non-Suicide Word Cloud"
      ],
      "metadata": {
        "id": "Zm5yEVj7o4yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonsuicide_word_scores_roberta = collect_ig_scores_roberta(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    correct_nonsuicide_samples,\n",
        "    target_class=0,\n",
        "    max_samples=50\n",
        ")\n",
        "\n",
        "plot_ig_wordcloud(\n",
        "    nonsuicide_word_scores_roberta,\n",
        "    title=\"IG-Weighted Word Cloud — Non-Suicide Class\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "y5nHlrZno6eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CONTRASTIVE INTEGRATED GRADIENTS (Suicide − Non-Suicide)"
      ],
      "metadata": {
        "id": "jV3u1zS_qKC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####IGcontrast​(w)=E[IG(w∣Suicide)]−E[IG(w∣Non-Suicide)]"
      ],
      "metadata": {
        "id": "YG9D8ncsretP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Contrastive IG collector"
      ],
      "metadata": {
        "id": "2LgykDmWs8mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def clean_roberta_token(token):\n",
        "    token = token.replace(\"Ġ\", \"\")\n",
        "    if token in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
        "        return \"\"\n",
        "    if all(c in string.punctuation for c in token):\n",
        "        return \"\"\n",
        "    return token.lower()\n",
        "\n",
        "\n",
        "def collect_ig_scores_roberta(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    correct_samples,\n",
        "    target_class,\n",
        "    max_samples=200,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    model.eval()\n",
        "    ig = IntegratedGradients(forward_func_roberta)\n",
        "\n",
        "    word_scores = defaultdict(float)\n",
        "    word_counts = defaultdict(int)\n",
        "\n",
        "    for sample_ids in correct_samples[:max_samples]:\n",
        "        sample_ids = sample_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = (sample_ids != tokenizer.pad_token_id).long()\n",
        "\n",
        "        baseline_ids = torch.full_like(\n",
        "            sample_ids, tokenizer.pad_token_id\n",
        "        ).to(device)\n",
        "\n",
        "        input_embeds = model.roberta.embeddings(sample_ids)\n",
        "        baseline_embeds = model.roberta.embeddings(baseline_ids)\n",
        "        input_embeds.requires_grad_(True)\n",
        "\n",
        "        attributions = ig.attribute(\n",
        "            inputs=input_embeds,\n",
        "            baselines=baseline_embeds,\n",
        "            additional_forward_args=(attention_mask,),\n",
        "            target=target_class\n",
        "        )\n",
        "\n",
        "        token_scores = attributions.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "        tokens = tokenizer.convert_ids_to_tokens(sample_ids.squeeze(0))\n",
        "\n",
        "        for tok, score in zip(tokens, token_scores):\n",
        "            tok = clean_roberta_token(tok)\n",
        "            if tok == \"\" or len(tok) < 3:\n",
        "                continue\n",
        "\n",
        "            word_scores[tok] += score\n",
        "            word_counts[tok] += 1\n",
        "\n",
        "    # Average IG per word\n",
        "    avg_word_scores = {\n",
        "        w: word_scores[w] / word_counts[w]\n",
        "        for w in word_scores\n",
        "        if word_counts[w] >= 2\n",
        "    }\n",
        "\n",
        "    return avg_word_scores\n"
      ],
      "metadata": {
        "id": "lFtMfCfmrhYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compute contrastive IG"
      ],
      "metadata": {
        "id": "OfetLM7DtJuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "suicide_ig = collect_ig_scores_roberta(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    correct_suicide_samples,\n",
        "    target_class=1,\n",
        "    max_samples=300\n",
        ")\n",
        "\n",
        "nonsuicide_ig = collect_ig_scores_roberta(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    correct_nonsuicide_samples,\n",
        "    target_class=0,\n",
        "    max_samples=300\n",
        ")\n",
        "\n",
        "contrastive_ig = {\n",
        "    w: suicide_ig[w] - nonsuicide_ig.get(w, 0.0)\n",
        "    for w in suicide_ig\n",
        "    if (suicide_ig[w] - nonsuicide_ig.get(w, 0.0)) > 0\n",
        "}\n"
      ],
      "metadata": {
        "id": "R5ftXfdYqLjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wc = WordCloud(\n",
        "    width=1400,\n",
        "    height=500,\n",
        "    background_color=\"white\",\n",
        "    colormap=\"Reds\",\n",
        "    max_words=120\n",
        ").generate_from_frequencies(contrastive_ig)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Contrastive Integrated Gradients (Suicide − Non-Suicide) — RoBERTa\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f5hwyjyftmg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Why this is strong academically\n",
        "\n",
        "Three levels of explainability was done:\n",
        "\n",
        "1. IG per sample (local)\n",
        "\n",
        "2. Aggregated IG (global)\n",
        "\n",
        "3. Contrastive IG (class-discriminative)"
      ],
      "metadata": {
        "id": "yQ9oraP1uc3n"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}